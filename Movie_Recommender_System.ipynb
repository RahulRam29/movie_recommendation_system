{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Libraries"
      ],
      "metadata": {
        "id": "G77PkmF4J0tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf                        #data automation, model tracking, performance monitoring, and model retraining\n",
        "from tensorflow import keras                   #used for distributed training of deep learning models\n",
        "from tensorflow.keras import layers, callbacks"
      ],
      "metadata": {
        "id": "CBjW_YFtKAMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Layer"
      ],
      "metadata": {
        "id": "95jSJ580KGoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "def input_100k():\n",
        "  global movies\n",
        "  movies = pd.read_csv('movies.csv')\n",
        "  global ratings\n",
        "  ratings = pd.read_csv('ratings.csv')\n"
      ],
      "metadata": {
        "id": "byaT_IXTKEiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common Layers\n",
        "\n",
        "In ratings.csv file we have 'userID', 'movieID', 'rating', 'timestamp'\n",
        "\n",
        "In movies.csv file we have 'movieID', 'title', 'genres'\n",
        "\n",
        "In movies.dat file, we have s.no and movie name, year, genre, children's or not"
      ],
      "metadata": {
        "id": "h9OagzZpKTU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def common_layer1():\n",
        "  # Preprocess data\n",
        "  unique_movie_ids = ratings['movieId'].unique()\n",
        "  unique_user_ids = ratings['userId'].unique()\n",
        "\n",
        "  # Create a mapping from movie and user ids to their respective indices in the input tensors\n",
        "  movie_to_idx = {movie_id: i for i, movie_id in enumerate(unique_movie_ids)}\n",
        "  user_to_idx = {user_id: i for i, user_id in enumerate(unique_user_ids)}\n",
        "\n",
        "  # Add the indices to the ratings dataframe\n",
        "  ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx)\n",
        "  ratings['user_idx'] = ratings['userId'].map(user_to_idx)\n",
        "\n",
        "  # Split data into training and test sets\n",
        "  global train_df\n",
        "  train_df = ratings.sample(frac=0.8, random_state=42)\n",
        "  global test_df\n",
        "  test_df = ratings.drop(train_df.index)\n",
        "\n",
        "  # Define model architecture\n",
        "  n_users = len(user_to_idx)\n",
        "  n_movies = len(movie_to_idx)\n",
        "  n_factors = 50\n",
        "\n",
        "  # Input layers\n",
        "  global movie_input\n",
        "  movie_input = keras.layers.Input(shape=[1], name='movie')\n",
        "  global user_input\n",
        "  user_input = keras.layers.Input(shape=[1], name='user')\n",
        "\n",
        "  # Embedding layers\n",
        "  movie_embedding = keras.layers.Embedding(n_movies, n_factors, name='movie_embedding')(movie_input)\n",
        "  user_embedding = keras.layers.Embedding(n_users, n_factors, name='user_embedding')(user_input)\n",
        "\n",
        "  # Reshape embedding layers for compatibility with convolutional layers\n",
        "  movie_embedding_reshaped = keras.layers.Reshape((1, n_factors, 1))(movie_embedding)\n",
        "  user_embedding_reshaped = keras.layers.Reshape((1, n_factors, 1))(user_embedding)\n",
        "\n",
        "  # Convolutional layers\n",
        "  conv1 = keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(movie_embedding_reshaped)\n",
        "  conv2 = keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(user_embedding_reshaped)\n",
        "  merged = keras.layers.concatenate([conv1, conv2])\n",
        "\n",
        "  return merged\n",
        "\n",
        "def common_layer2():\n",
        "  # Flatten layer\n",
        "  flatten = keras.layers.Flatten()(pool)\n",
        "\n",
        "  # Output layer\n",
        "  output = keras.layers.Dense(1, activation='relu')(flatten)\n",
        "\n",
        "  # Define model\n",
        "  model = keras.Model(inputs=[movie_input, user_input], outputs=output)\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
        "\n",
        "  # Train the model on the training set\n",
        "  global history\n",
        "  history = model.fit(\n",
        "    [train_df['movie_idx'].values, train_df['user_idx'].values], train_df['rating'].values,\n",
        "    epochs=20, batch_size=32, verbose=1,\n",
        "    validation_data=([test_df['movie_idx'].values, test_df['user_idx'].values], test_df['rating'].values),\n",
        "    callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
        "  )\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  test_loss, test_mae = model.evaluate([test_df['movie_idx'].values, test_df['user_idx'].values], test_df['rating'].values)\n",
        "  test_rmse = np.sqrt(test_loss)\n",
        "  print(\"Test RMSE:\", test_rmse)\n",
        "  print(\"Test MAE:\", test_loss)\n"
      ],
      "metadata": {
        "id": "bbxhuPZeKWZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting"
      ],
      "metadata": {
        "id": "rRVz0sFeKfxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot():\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Get training and test loss histories\n",
        "  training_loss = history.history['loss']\n",
        "\n",
        "  # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "  # Visualize loss history\n",
        "  plt.plot(epoch_count, training_loss, 'r')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show();"
      ],
      "metadata": {
        "id": "WpZuNYmTKg8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max Pooling for 100k dataset"
      ],
      "metadata": {
        "id": "rQlZMgKtLcQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_100k()\n",
        "merged=common_layer1()\n",
        "pool = keras.layers.MaxPooling2D(pool_size=(1, 2))(merged)\n",
        "common_layer2()\n",
        "plot()"
      ],
      "metadata": {
        "id": "cJl6TWPPLi0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot():\n",
        "  # Get training and validation loss histories\n",
        "  training_loss = history.history['loss']\n",
        "  validation_loss = history.history['val_loss']\n",
        "\n",
        "  # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "  # Visualize loss history\n",
        "  plt.plot(epoch_count, training_loss, 'r', label='Training Loss')\n",
        "  plt.plot(epoch_count, validation_loss, 'b', label='Validation Loss')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "UGnaISjxLnlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_100k()\n",
        "merged=common_layer1()\n",
        "pool = keras.layers.MaxPooling2D(pool_size=(1, 2))(merged)\n",
        "common_layer2()\n",
        "plot()"
      ],
      "metadata": {
        "id": "H2wfOtwpLpxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Pooling for 100K dataset"
      ],
      "metadata": {
        "id": "wpzwaRrsLsjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_100k()\n",
        "merged=common_layer1()\n",
        "pool = keras.layers.AveragePooling2D(pool_size=(1, 2))(merged)\n",
        "common_layer2()\n",
        "plot()"
      ],
      "metadata": {
        "id": "Wm5tI5PqLveV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CCF for 100k dataset\n"
      ],
      "metadata": {
        "id": "ZPp8lQx4L0ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_conv_filter(window_size=5):\n",
        "# Merge the two datasets on the 'movieId' column\n",
        "    data = pd.merge(ratings, movies, on='movieId')\n",
        "\n",
        "# Group the data by 'userId' and create a pivot table with 'movieId' as rows and 'userId' as columns\n",
        "    ratings_matrix = data.pivot_table(index='movieId', columns='userId', values='rating')\n",
        "\n",
        "  # Replace any missing values with zeros\n",
        "    ratings_matrix = ratings_matrix.fillna(0)\n",
        "\n",
        "  # Define a function to calculate the cross-convolution filter for a given movie\n",
        "\n",
        "    # Get the movie id from the user via the keyboard\n",
        "    movie_name = input(\"Enter a movie name: \")\n",
        "    movie_year = input(\"Enter movie year: \")\n",
        "    movie_name=movie_name+\" (\"+movie_year+\")\"\n",
        "    #movies = pd.read_csv('u.item', sep='|', encoding='latin-1', usecols=[0, 1], names=['movie_id', 'movie_title'])\n",
        "    movie_id = movies[movies['title'] == movie_name]['movieId'].values[0]\n",
        "\n",
        "    # Get the ratings for the given movie\n",
        "    movie_ratings = ratings_matrix.loc[movie_id].values\n",
        "\n",
        "    # Pad the ratings array with zeros on both sides\n",
        "    padded_ratings = np.pad(movie_ratings, (window_size//2, window_size//2), mode='constant', constant_values=0)\n",
        "\n",
        "    # Define the filters\n",
        "    h_filter = np.array([-1, 0, 1])\n",
        "    v_filter = np.array([[-1], [0], [1]])\n",
        "\n",
        "    # Calculate the horizontal convolution\n",
        "    h_convolved_ratings = np.convolve(padded_ratings, h_filter, mode='valid')\n",
        "\n",
        "    # Pad the horizontal convolved ratings array with zeros on both sides\n",
        "    padded_h_convolved_ratings = np.pad(h_convolved_ratings, (window_size//2, window_size//2), mode='constant', constant_values=0)\n",
        "\n",
        "    # Calculate the vertical convolution\n",
        "    v_convolved_ratings = np.convolve(padded_h_convolved_ratings, v_filter.flatten(), mode='valid')\n",
        "\n",
        "    # Get the top 10 highest values\n",
        "    top_indices = np.argsort(v_convolved_ratings)[-10:]\n",
        "\n",
        "    # Get the movie titles and scores\n",
        "    movie_indices = ratings_matrix.index[top_indices]\n",
        "    movie_scores = v_convolved_ratings[top_indices]\n",
        "    movie_titles = movies[movies['movieId'].isin(movie_indices)]['title'].values\n",
        "\n",
        "    # Print the recommended movies\n",
        "    print('Recommended movies for movieId {}:'.format(movie_id))\n",
        "    for i in range(len(movie_indices)):\n",
        "        print('{}, score: {:.2f}'.format(movie_titles[i], movie_scores[i]))\n",
        "\n",
        "\n",
        "# Example usage 100k\n",
        "input_100k()\n",
        "cross_conv_filter()\n"
      ],
      "metadata": {
        "id": "H04oLDXCL0-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}